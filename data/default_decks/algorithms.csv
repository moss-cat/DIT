front,back,deck
"Explain the time and space complexity of Fibonacci Heap's decrease-key operation compared to Binary Heap.","Fibonacci Heap has O(1) amortized time complexity for decrease-key vs O(log n) in Binary Heap. Space complexity is O(n) for both, but Fibonacci Heap has higher constant factors due to additional structural overhead.",algorithms
"Describe how the Aho-Corasick string matching algorithm works and its time complexity.","Aho-Corasick builds a finite state machine from patterns, then processes the text in a single pass. It combines ideas from KMP and trie data structures. Time complexity is O(n+m+z), where n is text length, m is total pattern length, and z is number of matches.",algorithms
"Explain how the Bellman-Ford algorithm can detect negative cycles and why this is important.","Bellman-Ford runs for |V|-1 iterations to find shortest paths. If running one more iteration changes any distance, a negative cycle exists. This is important because negative cycles mean no true shortest path exists (can keep reducing the path cost indefinitely).",algorithms
"Compare and contrast the Tarjan and Kosaraju algorithms for finding strongly connected components in a graph.","Both find SCCs in O(V+E) time. Kosaraju requires two DFS passes (one on original graph, one on transpose) while Tarjan uses a single DFS with more complex bookkeeping (low-link values). Tarjan is often more efficient in practice despite both having the same asymptotic complexity.",algorithms
"Explain how Suffix Arrays can be constructed in O(n log n) time and why this is faster than naive approaches.","Suffix Arrays can be constructed using the DC3/skew algorithm or SA-IS in O(n log n) time. This improves upon the naive O(n² log n) approach (sorting all suffixes directly). The key insight is using recursive doubling and exploiting previously computed ranks.",algorithms
"Describe how the A* search algorithm guarantees optimality and when its performance degrades to that of Dijkstra's algorithm.","A* guarantees optimality when the heuristic is admissible (never overestimates cost). It degrades to Dijkstra's when the heuristic is 0 everywhere or when nearly all nodes must be explored anyway. The algorithm's efficiency depends on how closely the heuristic approximates the true cost.",algorithms
"Explain how the Burrows-Wheeler Transform works and its applications in compression algorithms.","BWT rearranges characters to group similar contexts together without losing information. It works by sorting all rotations of the text and taking the last column. BWT enables efficient compression (used in bzip2) as it creates long runs of similar characters that compress well with move-to-front coding and Huffman/arithmetic coding.",algorithms
"Describe the internal mechanics of the Skip List data structure and analyze its expected performance.","Skip Lists maintain multiple layers of linked lists with exponentially decreasing node counts. Each higher layer 'skips' over more nodes. With randomization, operations achieve O(log n) expected time by allowing searches to quickly skip sections. Space complexity is O(n) as expected number of pointers is 2n.",algorithms
"Explain how the Fast Fourier Transform (FFT) achieves O(n log n) complexity for polynomial multiplication and its algorithmic importance.","FFT achieves O(n log n) by recursively breaking down the DFT calculation using the divide-and-conquer paradigm and exploiting symmetries in the complex roots of unity. This allows polynomial multiplication in O(n log n) instead of O(n²), enabling faster convolutions and large integer multiplication.",algorithms
"Describe how the Bloom filter data structure works, its space efficiency, and the tradeoff between false positive rate and memory usage.","A Bloom filter uses multiple hash functions mapping elements to bits in a bit array. All referenced bits are set on insert. For lookups, if any bit is 0, the element definitely isn't present; if all are 1, it's probably present. The false positive rate decreases exponentially with more bits per element, giving excellent space efficiency for approximate membership tests.",algorithms
"Explain how the Simplex algorithm works for linear programming and why it's efficient in practice despite exponential worst-case complexity.","Simplex traverses the vertices of the feasible region (a convex polytope) moving along edges that improve the objective function until finding an optimal vertex. Despite exponential worst-case complexity, it's typically O(m+n) iterations in practice due to the sparsity of real-world problems and the fact that worst-case inputs are rare in practical applications.",algorithms
"Describe how the Boyer-Moore string matching algorithm achieves sublinear time complexity in the average case.","Boyer-Moore achieves sublinear time by using two heuristics: bad character (skipping alignments where text characters don't match pattern) and good suffix (using information from previous matches). This allows it to skip large portions of the text, often examining only O(n/m) characters for a text of length n and pattern of length m.",algorithms
"Explain how the Held-Karp algorithm solves the Traveling Salesperson Problem and its time/space complexity.","Held-Karp uses dynamic programming to solve TSP. It computes optimal paths for all subsets of cities, building up from smaller to larger subsets. Time complexity is O(n²·2ⁿ) and space complexity is O(n·2ⁿ), making it feasible only for small instances, but it's the fastest known exact algorithm for TSP.",algorithms
"Describe how the HyperLogLog algorithm estimates cardinality with sublinear space complexity.","HyperLogLog estimates the number of unique elements by tracking the maximum number of leading zeros in hash values of elements. It divides inputs into buckets to reduce variance. With just O(log log n) bits per bucket and O(1/√m) error rate for m buckets, it can count billions of unique items using kilobytes instead of gigabytes.",algorithms
"Explain how the Splay Tree self-adjusting binary search tree works and why it achieves amortized O(log n) operations.","Splay trees move recently accessed elements to the root via a series of rotations called 'splaying.' While individual operations can take O(n) time, any sequence of m operations takes O(m log n) time amortized. This gives theoretical optimal performance for access patterns with locality and automatically adapts to usage patterns.",algorithms
"Describe the concept of Fractional Cascading and how it optimizes multiple binary searches across a collection of sorted arrays.","Fractional cascading accelerates multiple binary searches by augmenting each data structure with samples from others. After finding an element in one structure, its position in others can be found in O(1) time instead of O(log n). This reduces the time for k searches from O(k log n) to O(log n + k).",algorithms
"Explain how the Treap data structure combines binary search trees and heaps, and analyze its expected performance.","Treaps assign random priorities to keys and maintain both BST property by keys and heap property by priorities. The randomization ensures balanced trees with high probability. Operations take O(log n) expected time, and the structure is simpler to implement than AVL or Red-Black trees while offering similar performance guarantees.",algorithms
"Describe how the Fenwick Tree (Binary Indexed Tree) achieves efficient range sum queries and point updates.","Fenwick Trees represent cumulative frequency tables in a way that both queries and updates take O(log n) time. The key insight is using the binary representation of indices to store partial sums. Each node implicitly covers a range based on the least significant 1-bit in its index, allowing efficient implementation using bitwise operations.",algorithms
"Explain how the Karger's algorithm finds minimum cuts in graphs and its probability of success.","Karger's algorithm randomly contracts edges until only two vertices remain, which defines a cut. The probability of finding a minimum cut in one run is at least 2/(n(n-1)), so by running O(n² log n) times, we can find the minimum cut with high probability. Despite its simplicity, this randomized approach is one of the most practical for minimum cut problems.",algorithms
"Describe how the Z-algorithm computes the Z-array for string matching and its linear time complexity.","The Z-algorithm computes the Z-array, where Z[i] is the length of the longest substring starting at position i that matches the prefix of the string. It achieves O(n) time by maintaining a 'Z-box' (interval) and reusing previously computed values. This allows for linear-time string matching without preprocessing a separate pattern.",algorithms